{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SeaLLMs/SeaLLM3-7B-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "df = pd.read_json(\"data/train.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(20000, random_state=42)\n",
    "sample_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_segment</th>\n",
       "      <th>th_segment</th>\n",
       "      <th>review_star</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When this arrived I decided to read them right...</td>\n",
       "      <td>เมื่อสิ่งนี้มาถึงฉันตัดสินใจที่จะอ่านพวกเขาทัน...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was looking for a replacement of the old sho...</td>\n",
       "      <td>ฉันกำลังมองหาการเปลี่ยนหัวฝักบัวเก่าที่มาพร้อม...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was looking for something more sturdy and st...</td>\n",
       "      <td>ฉันกำลังมองหาบางสิ่งที่แข็งแกร่งและแข็งแกร่งกว...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought this book was supposed to be about t...</td>\n",
       "      <td>ฉันคิดว่าหนังสือเล่มนี้ควรเกี่ยวกับคริสตจักรยุ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was expecting a book for the &amp;#34;normal&amp;#34...</td>\n",
       "      <td>ฉันคาดหวังว่าหนังสือสำหรับผู้ใช้ &amp;quot;ปกติ&amp;qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>I purchased my Hoover SteamVac Carpet Cleaner ...</td>\n",
       "      <td>ฉันซื้อ Hoover SteamVac Carpet Cleaner ประมาณห...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>I wish they made more of these without the vel...</td>\n",
       "      <td>ฉันหวังว่าพวกเขาจะทำสิ่งเหล่านี้ได้มากขึ้นโดยไ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>It is a great album with awesome music and I l...</td>\n",
       "      <td>มันเป็นอัลบั้มที่ยอดเยี่ยมพร้อมกับเพลงที่ยอดเย...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>I purchased the Nikon P100 with a UV filter. W...</td>\n",
       "      <td>ฉันซื้อ Nikon P100 พร้อมฟิลเตอร์ UV เมื่อใช้มั...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>In The Year 2512, a mysterious illness is swee...</td>\n",
       "      <td>ในปี พ.ศ. 2512 มีเหตุการณ์ลึกลับเกิดขึ้นมากมาย...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              en_segment  \\\n",
       "0      When this arrived I decided to read them right...   \n",
       "1      I was looking for a replacement of the old sho...   \n",
       "2      I was looking for something more sturdy and st...   \n",
       "3      I thought this book was supposed to be about t...   \n",
       "4      I was expecting a book for the &#34;normal&#34...   \n",
       "...                                                  ...   \n",
       "19995  I purchased my Hoover SteamVac Carpet Cleaner ...   \n",
       "19996  I wish they made more of these without the vel...   \n",
       "19997  It is a great album with awesome music and I l...   \n",
       "19998  I purchased the Nikon P100 with a UV filter. W...   \n",
       "19999  In The Year 2512, a mysterious illness is swee...   \n",
       "\n",
       "                                              th_segment  review_star  correct  \n",
       "0      เมื่อสิ่งนี้มาถึงฉันตัดสินใจที่จะอ่านพวกเขาทัน...            5        0  \n",
       "1      ฉันกำลังมองหาการเปลี่ยนหัวฝักบัวเก่าที่มาพร้อม...            5        0  \n",
       "2      ฉันกำลังมองหาบางสิ่งที่แข็งแกร่งและแข็งแกร่งกว...            1        0  \n",
       "3      ฉันคิดว่าหนังสือเล่มนี้ควรเกี่ยวกับคริสตจักรยุ...            1        0  \n",
       "4      ฉันคาดหวังว่าหนังสือสำหรับผู้ใช้ &quot;ปกติ&qu...            1        0  \n",
       "...                                                  ...          ...      ...  \n",
       "19995  ฉันซื้อ Hoover SteamVac Carpet Cleaner ประมาณห...            4        0  \n",
       "19996  ฉันหวังว่าพวกเขาจะทำสิ่งเหล่านี้ได้มากขึ้นโดยไ...            3        0  \n",
       "19997  มันเป็นอัลบั้มที่ยอดเยี่ยมพร้อมกับเพลงที่ยอดเย...            1        0  \n",
       "19998  ฉันซื้อ Nikon P100 พร้อมฟิลเตอร์ UV เมื่อใช้มั...            1        1  \n",
       "19999  ในปี พ.ศ. 2512 มีเหตุการณ์ลึกลับเกิดขึ้นมากมาย...            3        0  \n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|>', '<|endoftext|>', '<|im_start|>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151645"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [151643, 220, 151644], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<|endoftext|> <|im_start|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [143126, 23271, 125136, 28319], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"สวัสดี\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.df[\"en_segment\"][idx]\n",
    "        tgt = self.df[\"th_segment\"][idx]\n",
    "\n",
    "        src_token = tokenizer(src, truncation=True, padding=\"max_length\", max_length=128)[\"input_ids\"]\n",
    "        tgt_token = tokenizer(tgt, truncation=True, padding=\"max_length\", max_length=128)[\"input_ids\"]\n",
    "\n",
    "        return torch.tensor(src_token), torch.tensor(tgt_token)\n",
    "\n",
    "dataset = CustomDataset(sample_df)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "dataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model != num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_score, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_lenght, d_model = x.size()\n",
    "        return x.view(batch_size, seq_lenght, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_lenght, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_lenght, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "\n",
    "# Create an instance of MultiHeadAttention\n",
    "multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Generate random input tensors\n",
    "Q = torch.randn(batch_size, seq_length, d_model)\n",
    "K = torch.randn(batch_size, seq_length, d_model)\n",
    "V = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "# # Call the forward method and print the output\n",
    "output = multihead_attn(Q, K, V)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([4, 10, 512])\n",
      "Output Shape: torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512  \n",
    "d_ff = 2048 \n",
    "batch_size = 4 \n",
    "seq_len = 10\n",
    "\n",
    "# Create an instance of the class\n",
    "model = PositionWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "# Generate some random input data\n",
    "random_input = torch.randn(batch_size, seq_len, d_model) \n",
    "\n",
    "# Pass the data through the model\n",
    "output = model(random_input)\n",
    "\n",
    "# Print the shapes of the input and output tensors \n",
    "print(\"Input Shape:\", random_input.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_lenght):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_lenght, d_model) \n",
    "        position = torch.arange(0, max_seq_lenght, dtype=torch.float).unsqueeze(1)\n",
    "        dev_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * dev_term)\n",
    "        pe[:, 1::2] = torch.cos(position * dev_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "max_seq_lenght = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = PositionalEncoding(d_model, max_seq_lenght)\n",
    "\n",
    "random_input = torch.randn(batch_size, max_seq_lenght, d_model)\n",
    "\n",
    "output = model(random_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        att_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(att_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "seq_length = 10\n",
    "\n",
    "model = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "random_input = torch.randn(batch_size, seq_length, d_model)\n",
    "mask = torch.ones(batch_size, seq_length, seq_length)\n",
    "\n",
    "output = model(random_input, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "seq_length = 10\n",
    "\n",
    "model = DecderLayer(d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, d_model)\n",
    "enc_model = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "src_mask = torch.ones(batch_size, seq_length, seq_length)\n",
    "enc_output = enc_model(x, mask)\n",
    "tgt_mask = torch.ones(batch_size, seq_length, seq_length)\n",
    "\n",
    "output = model(x, enc_output, src_mask, tgt_mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 tgt_vocab_size, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 num_layers, \n",
    "                 d_ff,\n",
    "                 max_seq_lenght,\n",
    "                 dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_lenght)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embeded = self.dropout(self.positional_encoding(self.encoder_embed(src)))\n",
    "        tgt_embeded = self.dropout(self.positional_encoding(self.decoder_embed(tgt)))\n",
    "\n",
    "        enc_output = src_embeded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "\n",
    "        dec_output = tgt_embeded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = tokenizer.vocab_size+1000\n",
    "tgt_vocab_size = tokenizer.vocab_size+1000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# # Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (16, max_seq_length))  # (batch_size, seq_length)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, (16, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is running on cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "opt = optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "# src_data = src_data.to(device)\n",
    "# tgt_data = tgt_data.to(device)\n",
    "\n",
    "print(\"model is running on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "def eval(model=None, loader=None):\n",
    "    y_true = [\"สวัสดีครับชมรมคนชอบหมี\"]\n",
    "    y_pred = [\"สวัสดีครับชมรมคนชอบของสะสม\"]\n",
    "    \n",
    "    y_true = [tokenizer.batch_decode(tokenizer(i, truncation=True, padding=\"max_length\", max_length=128)[\"input_ids\"]) for i in y_true]\n",
    "    y_pred = [tokenizer.batch_decode(tokenizer(i, truncation=True, padding=\"max_length\", max_length=128)[\"input_ids\"]) for i in y_pred]\n",
    "    \n",
    "    y_pred = np.array(y_pred).squeeze()\n",
    "    \n",
    "    \n",
    "    bleu_score = sentence_bleu(y_true, y_pred)\n",
    "    \n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(tokenizer(i, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m y_pred]\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_pred)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 14\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43msentence_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bleu_score\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[0;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_bleu\u001b[39m(\n\u001b[1;32m     21\u001b[0m     references,\n\u001b[1;32m     22\u001b[0m     hypothesis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     auto_reweigh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m ):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    :rtype: float / list(float)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reweigh\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[1;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[1;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log, exp\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, order):\n",
    "    \"\"\"\n",
    "    Given a string `text` and an integer `order`, returns a Counter object containing\n",
    "    the frequency counts of all ngrams of size `order` in the string.\n",
    "    \"\"\"\n",
    "    ngrams = Counter()\n",
    "\n",
    "    words = text.split()\n",
    "    for i in range(len(words)- order+1):\n",
    "      ngram = \" \". join(words[i: i + order])\n",
    "      ngrams[ngram] += 1\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "def calculate_bleu(hypothesis, references):\n",
    "    \n",
    "    bleu=0\n",
    "    p1=0\n",
    "    p2=0\n",
    "    p3=0\n",
    "    p4=0\n",
    "    bp=1\n",
    "\n",
    "    # 1. Find the closest reference to the hypothesis\n",
    "    closest_size=100000\n",
    "    closest_ref=[]\n",
    "\n",
    "    for ref in references:\n",
    "      ref_size = len(ref)\n",
    "      if abs(len(hypothesis) - ref_size) < closest_size:\n",
    "        closest_size = abs(len(hypothesis) - ref_size)\n",
    "        closest_ref = ref\n",
    "        pass\n",
    "\n",
    "    # 2. Calculating pn\n",
    "    pns=[]\n",
    "    for order in range(1,5):\n",
    "      # calculate intersection and union of n-grams\n",
    "      # hint: use the get_ngrams function you implemented\n",
    "      # calculate pn for each order\n",
    "        hyp_ngrams = get_ngrams(hypothesis, order)\n",
    "        hyp_count = Counter(hyp_ngrams)\n",
    "        closest_ref_ngrams = get_ngrams(closest_ref, order)\n",
    "        closest_ref_count = Counter(closest_ref_ngrams)\n",
    "        intersection_count = dict(hyp_count & closest_ref_count)\n",
    "        intersection_size = sum(intersection_count.values())\n",
    "        hyp_size = max(len(hyp_ngrams), 1)\n",
    "        p_n = intersection_size / hyp_size\n",
    "        pns.append(p_n)\n",
    "        pass\n",
    "\n",
    "    # 3. Calculating the brevity penalty\n",
    "    bp=1\n",
    "    c=len(hypothesis)\n",
    "    r=min(abs(len(ref) - c) for ref in references)\n",
    "    if c > r:\n",
    "      bp = 1.0\n",
    "    else:\n",
    "      bp = exp(1 - r / c)\n",
    "\n",
    "    # 4. Calculating the BLEU score\n",
    "    weights = [0.25] * 4\n",
    "    bleu=bp * exp(sum(w * log(p_n) for w, p_n in zip(weights, pns)))    \n",
    "    \n",
    "    # Assigning values to p1, p2, p3, p4!\n",
    "    p1, p2, p3, p4 = pns\n",
    "\n",
    "    \n",
    "    # Do not change the variable name\n",
    "    return bleu, p1, p2, p3, p4, bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis=\"Abandon all hope , ye who enter here\"\n",
    "references=[\"All hope abandon , ye who enter here\", \"All hope abandon , ye who enter in !\", \"Leave every hope, ye that enter\", \"Leave all hope , ye that enter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.541\n"
     ]
    }
   ],
   "source": [
    "bleu, p1, p2, p3, p4, bp=calculate_bleu(hypothesis, references)\n",
    "print(\"BLEU: %.3f\" % bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:56<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.22751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:57<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Loss: 0.01937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(2):\n",
    "    transformer.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    for src, tgt in tqdm(dataLoader):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        output = transformer(src, tgt[:, 1:])\n",
    "        loss = loss_fn(output.contiguous().view(-1, tgt_vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02d}, Loss: {loss_sum / len(dataLoader):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embed): Embedding(152643, 512)\n",
       "  (decoder_embed): Embedding(152643, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=152643, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, src_raw, max_lenght=128, sos_token=151644, eos_token=151643):\n",
    "    model.eval()\n",
    "    model = model.to(\"cpu\")\n",
    "    src = tokenizer(src_raw, truncation=True, padding=\"max_length\", max_length=128)[\"input_ids\"]\n",
    "    src = torch.tensor([src])\n",
    "    src_mask, _ = model.generate_mask(src, src)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_embeded = model.encoder_embed(src)\n",
    "        src_embeded = model.positional_encoding(src_embeded)\n",
    "        enc_output = src_embeded\n",
    "        for enc_layer in model.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "    batch_size = src.size(0)\n",
    "    tgt_tokens = torch.ones(batch_size, 1).long().to(src.device)\n",
    "    \n",
    "    for _ in range(max_lenght-1):\n",
    "        _, tgt_mask = model.generate_mask(src, tgt_tokens)\n",
    "        tgt_embeded = model.decoder_embed(tgt_tokens)\n",
    "        tgt_embeded = model.positional_encoding(tgt_embeded)\n",
    "        dec_output = tgt_embeded\n",
    "        \n",
    "        for dec_layer in model.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "    \n",
    "        output = model.fc(dec_output[:, -1, :])\n",
    "        \n",
    "        _, next_token = torch.max(output, dim=1)\n",
    "        \n",
    "        tgt_tokens = torch.cat([tgt_tokens, next_token.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        if(next_token == eos_token).all():\n",
    "            break\n",
    "    \n",
    "    return tgt_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = predict(transformer, \"Best value in a hard to obtain item. The item was new with the box.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220,\n",
       "         220, 220]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"                                                                                                                               '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.....\n"
     ]
    }
   ],
   "source": [
    "models.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
